#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Promotion Cluster Evaluation Script

This script evaluates the quality and characteristics of promotion clusters created by the
clustering_promos.py script. It computes standard clustering metrics (Silhouette score)
and generates detailed profiles for each cluster, including estimated promotion uplift.

Key Features:
- Evaluates clustering quality with Silhouette scores using Gower distance
- Generates statistical profiles for each promotion cluster
- Calculates estimated sales uplift per cluster by comparing to baseline sales
- Creates both markdown and CSV reports with detailed cluster statistics
- Supports cluster quality assessment and business interpretation

Inputs:
- promo_clustered.csv: Output from clustering_promos.py with cluster assignments
- tunable_stl_promo_flagged.csv: Original promotion-flagged data with all sales records

Outputs:
- cluster_eval_report.md: Markdown report with cluster evaluation and profiles
- cluster_stats.csv: CSV file with detailed cluster statistics for further analysis

Usage:
    python cluster_evaluation.py
"""

import pandas as pd
import numpy as np
import os.path
from sklearn.metrics import silhouette_score
import gower
from tabulate import tabulate

def load_clustered_data():
    """
    Load the clustered promotion data generated by clustering_promos.py.
    
    Returns:
        pd.DataFrame: DataFrame containing promotion records with cluster assignments
    """
    input_file = "tunable_promo_clustered.csv"
    if not os.path.exists(input_file):
        raise FileNotFoundError(f"Clustered data file '{input_file}' not found! Run clustering_promos.py first.")
        
    print(f"Loading clustered promotion data from {input_file}...")
    df = pd.read_csv(input_file)
    
    print(f"Loaded {len(df):,} promotion records with {df['promo_cluster'].nunique()} clusters")
    print(f"Columns: {', '.join(df.columns)}")
    
    # Validate that the essential columns are present
    required_cols = ['Forecasting Group', 'Country', 'promo_cluster', 'Sales_volume']
    missing = [col for col in required_cols if col not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns: {', '.join(missing)}")
        
    return df

def load_full_data():
    """
    Load the full promotion-flagged sales data to calculate baseline sales.
    This dataset includes both promotional and non-promotional periods.
    
    Returns:
        pd.DataFrame: Full sales dataset with promotion flags
    """
    input_file = "tunable_stl_promo_flagged.csv"
    if not os.path.exists(input_file):
        raise FileNotFoundError(f"Full data file '{input_file}' not found! Run tunable_stl_promo_detection.py first.")
        
    print(f"Loading full sales dataset from {input_file}...")
    full_df = pd.read_csv(input_file)
    
    print(f"Loaded {len(full_df):,} total sales records")
    
    # Validate that promotion flag column exists
    if 'LikelyPromo' not in full_df.columns:
        raise ValueError("Missing 'LikelyPromo' column in the full dataset")
        
    # Count of promotional vs non-promotional records
    promo_count = full_df['LikelyPromo'].sum()
    print(f"Promotional periods: {promo_count:,} ({promo_count/len(full_df):.1%})")
    print(f"Non-promotional periods: {len(full_df) - promo_count:,} ({1 - promo_count/len(full_df):.1%})")
    
    return full_df

def compute_silhouette(df, features):
    """
    Compute the Silhouette score for the clustering results using Gower distance.
    
    The Silhouette score measures how similar each point is to its own cluster compared to other clusters.
    Higher values (closer to 1) indicate well-separated clusters, while values near 0 indicate overlapping clusters.
    Negative values suggest data points might be assigned to the wrong clusters.
    
    Args:
        df (pd.DataFrame): DataFrame with clustered promotion data
        features (list): List of features used for clustering evaluation
        
    Returns:
        float or None: Silhouette score (between -1 and 1) if calculable, None otherwise
    """
    print("\nComputing Gower distance for Silhouette score...")
    
    # Create a working copy to avoid modifying the original dataframe
    df_copy = df.copy()
    
    # Ensure correct data types for Gower distance calculation
    for col in features:
        if col not in df_copy.columns:
            print(f"Warning: Feature '{col}' not found in data, skipping")
            continue
            
        # Convert integer types to float for distance calculation
        if df_copy[col].dtype.name.startswith("UInt") or df_copy[col].dtype.name.startswith("Int"):
            df_copy[col] = df_copy[col].astype(float)
        # Convert categorical to object type
        if df_copy[col].dtype.name == "category":
            df_copy[col] = df_copy[col].astype(object)
            
    # Compute Gower distance matrix
    print(f"Computing distance matrix for {len(df_copy):,} records using features: {', '.join(features)}")
    try:
        gower_dist = gower.gower_matrix(df_copy[features]).astype("float64")
    except Exception as e:
        print(f"❌ Error computing Gower distance: {e}")
        return None
    
    # Get cluster labels and filter out noise points (cluster -1)
    labels = df_copy["promo_cluster"]
    mask = labels != -1  # Exclude noise points
    
    # Check if we have enough clusters for silhouette calculation
    if mask.sum() < 2 or len(set(labels[mask])) < 2:
        print("❌ Not enough valid clusters to compute Silhouette score.")
        print(f"  - Records excluding noise: {mask.sum():,}")
        print(f"  - Unique clusters excluding noise: {len(set(labels[mask]))}")
        return None

    # Compute silhouette score using precomputed distance
    print(f"Computing Silhouette score for {mask.sum():,} records across {len(set(labels[mask]))} clusters...")
    score = silhouette_score(gower_dist[mask][:, mask], labels[mask], metric="precomputed")
    
    # Interpret the score
    interpretation = "excellent" if score > 0.7 else "good" if score > 0.5 else "fair" if score > 0.3 else "poor"
    print(f"✅ Silhouette Score (excluding noise): {round(score, 4)} - {interpretation} cluster separation")
    
    return score

def compute_cluster_profiles(df, full_df):
    """
    Compute detailed statistical profiles for each promotion cluster.
    
    This function calculates key metrics for each cluster including:
    - Record count per cluster
    - Sales volume statistics
    - Seasonal distribution (months)
    - Geographic distribution (countries)
    - Estimated promotion uplift (compared to baseline sales)
    
    Args:
        df (pd.DataFrame): DataFrame with clustered promotion data
        full_df (pd.DataFrame): Full sales dataset with both promo and non-promo periods
        
    Returns:
        pd.DataFrame: DataFrame with detailed statistics for each cluster
    """
    print("\n--- Computing Cluster Profiles ---")
    
    # Calculate basic statistics per cluster
    print("Calculating basic cluster statistics...")
    cluster_stats = df.groupby("promo_cluster").agg({
        "Forecasting Group": "count",
        "Sales_volume": ["mean", "std"],
        "Month": "nunique",
        "Country": "nunique"  # Remove the lambda, just use "nunique" directly
    })
    
    # Calculate additional seasonal metrics if week information is available
    seasonal_cols = []
    if "WeekOfYear" in df.columns:
        print("Adding week-of-year distribution statistics...")
        week_stats = df.groupby("promo_cluster")["WeekOfYear"].agg(["mean", "std"]).rename(
            columns={"mean": "avg_week", "std": "week_std"}
        )
        seasonal_cols = ["avg_week", "week_std"]
    else:
        week_stats = pd.DataFrame()

    # Calculate estimated uplift compared to baseline sales
    print("Computing promotion uplift estimates...")
    # Filter the full dataset to get only non-promotional periods as baseline
    baseline_df = full_df[full_df["LikelyPromo"] == 0]
    
    # Calculate average baseline sales for each product-country pair
    print("Calculating baseline sales for each product-country combination...")
    baseline_avg = baseline_df.groupby(
        ["Forecasting Group", "Country"]
    )["Sales_volume"].mean().rename("baseline_sales")
    
    # Merge baseline sales with the clustered data
    print("Merging baseline sales with promotion data...")
    df = df.merge(baseline_avg, on=["Forecasting Group", "Country"], how="left")
    
    # Calculate uplift as the difference between promotional sales and baseline sales
    df["estimated_uplift"] = df["Sales_volume"] - df["baseline_sales"]
    df["uplift_pct"] = (df["Sales_volume"] / df["baseline_sales"]) - 1
    
    # Calculate uplift statistics per cluster
    uplift_stats = df.groupby("promo_cluster").agg({
        "estimated_uplift": ["mean", "std", "count"],
        "uplift_pct": ["mean", "median"]
    })
    
    # Flatten column names and format
    uplift_stats.columns = ["_".join(col).strip() for col in uplift_stats.columns.values]
    
    # Add percentile data for more detailed uplift distribution
    pct_stats = df.groupby("promo_cluster")["uplift_pct"].quantile([0.25, 0.75]).unstack()
    pct_stats.columns = [f"uplift_pct_{int(q*100)}" for q in pct_stats.columns]
    
    # Flatten and rename column names for better readability
    flat_stats = cluster_stats.copy()
    flat_stats.columns = ["_".join(col).strip() for col in flat_stats.columns.values]
    
    # Combine all statistics into a single result dataframe
    print("Compiling final cluster profiles...")
    result = flat_stats.join([uplift_stats, pct_stats])
    if not week_stats.empty:
        result = result.join(week_stats)
        
    # Add descriptive labels for the clusters
    result["cluster_type"] = "Regular"
    # Identify noise points
    result.loc[result.index == -1, "cluster_type"] = "Noise"
    # High uplift clusters (>50% average uplift)
    result.loc[(result.index != -1) & (result["uplift_pct_mean"] > 0.5), "cluster_type"] = "High Uplift"
    # Large clusters (>10% of non-noise points)
    non_noise_threshold = 0.1 * (df["promo_cluster"] != -1).sum()
    result.loc[(result.index != -1) & (result["Forecasting Group_count"] > non_noise_threshold), "cluster_type"] = "Large"
    
    # Reorder columns and reset index for better display
    result.reset_index(inplace=True)
    
    # Print a summary of the results
    print("\nCluster Profile Summary:")
    print(result[["promo_cluster", "cluster_type", "Forecasting Group_count", "uplift_pct_mean", "Country_nunique"]])
    
    return result

def save_report(score, stats_df, output_path="cluster_eval_report.md", csv_output="cluster_stats.csv"):
    """
    Save cluster evaluation results to both markdown and CSV formats.
    
    Creates a detailed markdown report with cluster quality metrics and profiles,
    as well as a CSV file with all cluster statistics for further analysis.
    
    Args:
        score (float or None): Silhouette score for the clustering
        stats_df (pd.DataFrame): DataFrame with cluster statistics
        output_path (str): Path for the markdown report output
        csv_output (str): Path for the CSV statistics output
        
    Returns:
        None
    """
    print(f"\nSaving evaluation results to {output_path} and {csv_output}...")
    
    # Generate the markdown report
    with open(output_path, "w") as f:
        # Title and overview
        f.write("# Promotion Cluster Evaluation Report\n\n")
        f.write(f"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\n\n")
        
        # Cluster quality metrics
        f.write("## Clustering Quality Metrics\n\n")
        if score is not None:
            interpretation = "excellent" if score > 0.7 else "good" if score > 0.5 else "fair" if score > 0.3 else "poor"
            f.write(f"**Silhouette Score (excluding noise):** {round(score, 4)} ({interpretation} cluster separation)\n\n")
        else:
            f.write("**Silhouette Score:** Could not be calculated (insufficient clusters or data)\n\n")

        # Total counts summary
        total_records = stats_df["Forecasting Group_count"].sum()
        noise_records = stats_df.loc[stats_df["promo_cluster"] == -1, "Forecasting Group_count"].sum() \
            if -1 in stats_df["promo_cluster"].values else 0
        noise_pct = 100 * noise_records / total_records if total_records > 0 else 0
            
        f.write(f"**Total Records:** {total_records:,}\n\n")
        f.write(f"**Noise Points:** {noise_records:,} ({noise_pct:.1f}%)\n\n")
        f.write(f"**Clusters Found:** {len(stats_df[stats_df['promo_cluster'] != -1])}\n\n")
        
        # Cluster profiles
        f.write("## Cluster Profiles with Estimated Uplift\n\n")
        f.write("This table shows key characteristics of each detected promotion cluster, including:\n\n")
        f.write("- **Count:** Number of promotions in each cluster\n")
        f.write("- **Sales Statistics:** Average sales and standard deviation\n")
        f.write("- **Seasonality:** Number of unique months when these promotions occur\n")
        f.write("- **Geography:** Number of countries where these promotions appear\n")
        f.write("- **Uplift:** Estimated sales increase compared to baseline\n\n")
        
        # Format the table with better column headers for display
        display_df = stats_df.copy()
        if 'cluster_type' in display_df.columns:
            display_df = display_df[['promo_cluster', 'cluster_type', 'Forecasting Group_count', 
                                    'Sales_volume_mean', 'uplift_pct_mean', 'Country_nunique', 'Month_nunique']]
            display_df.columns = ['Cluster', 'Type', 'Count', 'Avg Sales', 'Avg Uplift %', 'Countries', 'Months']
        
        f.write(tabulate(display_df, headers='keys', tablefmt='github', showindex=False))
        
        # Add interpretation guidelines
        f.write("\n\n## Interpretation Guidelines\n\n")
        f.write("- **High Uplift Clusters:** Promotions with >50% average sales increase\n")
        f.write("- **Large Clusters:** Common promotion patterns (>10% of all promotions)\n")
        f.write("- **Noise:** Outlier promotions that don't fit well in any cluster\n")
        f.write("- **Regular:** Standard promotion patterns\n\n")
        f.write("Higher silhouette scores indicate better-defined, more distinct clusters.\n")

    # Save the detailed statistics CSV
    stats_df.to_csv(csv_output, index=False)
    
    print(f"✅ Report saved to {output_path}")
    print(f"✅ Detailed statistics saved to {csv_output}")

def main():
    """
    Main execution function for promotion cluster evaluation.
    
    This function orchestrates the entire evaluation process:
    1. Loads clustered promotion data and full sales data
    2. Computes clustering quality metrics (Silhouette score)
    3. Generates detailed cluster profiles with statistics
    4. Saves evaluation results to markdown and CSV formats
    """
    print("\n===== Promotion Cluster Evaluation =====\n")
    
    # Define features used for clustering evaluation
    # These should match the features used in clustering_promos.py
    features = [
        "Category",      # Product category (categorical)
        "Country",      # Country (categorical)
        "Month",        # Month of year (temporal feature, numeric)
        "WeekOfYear",   # Week of year (temporal feature, numeric)
        "Sales_volume"  # Sales volume (numeric)
    ]
    print(f"Using features for evaluation: {', '.join(features)}")

    # Load required datasets
    try:
        promo_df = load_clustered_data()
        full_df = load_full_data()
    except Exception as e:
        print(f"\n❌ Error loading required data: {e}")
        print("Please run the prerequisite scripts first:")
        print("1. tunable_stl_promo_detection.py")
        print("2. clustering_promos.py")
        return

    # Compute clustering quality metrics
    try:
        score = compute_silhouette(promo_df, features)
    except Exception as e:
        print(f"\n❌ Error computing silhouette score: {e}")
        score = None

    # Generate cluster profiles
    try:
        stats_df = compute_cluster_profiles(promo_df, full_df)
    except Exception as e:
        print(f"\n❌ Error computing cluster profiles: {e}")
        print("Skipping report generation.")
        return
        
    # Save evaluation results
    try:
        output_dir = "reports"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        save_report(score, stats_df, 
                  output_path=os.path.join(output_dir, "cluster_eval_report.md"),
                  csv_output=os.path.join(output_dir, "cluster_stats.csv"))
        
        print("\n===== Evaluation Complete =====")
    except Exception as e:
        print(f"\n❌ Error saving evaluation report: {e}")
        print("Evaluation completed but could not save reports.")

if __name__ == "__main__":
    main()

